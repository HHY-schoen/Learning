{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出所有已安裝的 Python 套件\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(host=<your_port>, timeout=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model= deepseek-r1:70b , families= ['llama'] , parameter_size= 70.6B\n",
      "model= llama3.3:70b , families= ['llama'] , parameter_size= 70.6B\n",
      "model= gemma2:27b , families= ['gemma2'] , parameter_size= 27.2B\n",
      "model= llama3.2-vision:90b , families= ['mllama', 'mllama'] , parameter_size= 87.7B\n"
     ]
    }
   ],
   "source": [
    "for model in client.list()['models']:\n",
    "    print('model=', model['model'], ', families=', model['details']['families'], ', parameter_size=', model['details']['parameter_size'])\n",
    "\n",
    "# 選擇最小的 gemma2:27b 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paragraph(filename):\n",
    "    with open(filename) as f:\n",
    "        return [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 避免每次都重算，把計算好的向量存起來\n",
    "def calc_embedings(paragraphs):\n",
    "    return [\n",
    "        client.embeddings(model=\"gemma2:27b\", prompt=data)[\"embedding\"]  # 把每個元素轉換向量\n",
    "        for data in paragraphs\n",
    "    ]\n",
    "\n",
    "def cache_embeddings(filename, paragraphs):\n",
    "    embedding_file = f\"/cache/{filename}.json\"\n",
    "\n",
    "    if os.path.isfile(os.getcwd() + embedding_file):\n",
    "        with open(os.getcwd() + embedding_file) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    os.makedirs(os.path.join(os.getcwd(), \"cache\"), exist_ok=True)\n",
    "\n",
    "    embeddings = calc_embedings(paragraphs)\n",
    "\n",
    "    with open(os.getcwd()+embedding_file, \"w\") as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算向量相似度 (計算在向量們的距離)\n",
    "def calc_similar_vectors(v, vectors):\n",
    "    v_norm = np.linalg.norm(v)  # 計算向量的範數（長度/大小）\n",
    "    scores = [np.dot(v, item) / (v_norm * np.linalg.norm(item)) for item in vectors]  # 計算每個詞的餘弦相似度，內積(dot)可用於計算兩個向量的夾角（餘弦相似度）\n",
    "    return sorted(enumerate(scores), reverse=True, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.16556577858726831),\n",
       " (0, 0.040011433594213955),\n",
       " (2, 0.018359225964453264)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = client.embeddings(model=\"gemma2:27b\", prompt=\"天氣\")\n",
    "file = [client.embeddings(model=\"gemma2:27b\", prompt=word).embedding for word in [\"氣象局\", \"下雨天\", \"氣溫\"]]\n",
    "v_norm = np.linalg.norm(input.embedding)\n",
    "scores = [np.dot(input.embedding, item) / (v_norm * np.linalg.norm(item)) for item in file] \n",
    "sorted(enumerate(scores), reverse=True, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608\n",
      "4608\n",
      "4608\n"
     ]
    }
   ],
   "source": [
    "for f in file:\n",
    "    print(len(f))  # gemma2:27b 的詞向量長度為 4608 維"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作如下\n",
    "def parse_paragraph(filename):\n",
    "    with open(filename) as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def cache_embeddings(filename, paragraphs):\n",
    "    embedding_file = f\"/cache/{filename}.json\"\n",
    "    if os.path.isfile(os.getcwd() + embedding_file):\n",
    "        with open(os.getcwd() + embedding_file) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    os.makedirs(os.path.join(os.getcwd(), \"cache\"), exist_ok=True)\n",
    "\n",
    "    embeddings = calc_embedings(paragraphs)\n",
    "\n",
    "    with open(os.getcwd()+embedding_file, \"w\") as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def calc_similar_vectors(v, vectors):\n",
    "    v_norm = np.linalg.norm(v)\n",
    "    scores = [np.dot(v, item) / (v_norm * np.linalg.norm(item)) for item in vectors]\n",
    "    return sorted(enumerate(scores), reverse=True, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "paragraphs = parse_paragraph(\"about.txt\")  # 若文檔有更新要刪掉再重跑，不然只要有檔案就會不重新embedding\n",
    "embeddings = cache_embeddings(\"about.txt\", paragraphs)\n",
    "\n",
    "prompt = input(\"請輸入問題... (輸入'q'離開)\")\n",
    "while prompt.lower() != \"q\":\n",
    "    prompt_embedding = client.embeddings(model=\"gemma2:27b\", prompt=prompt)[\"embedding\"]\n",
    "    similar_vectors = calc_similar_vectors(prompt_embedding, embeddings)[:3]\n",
    "\n",
    "    system_prompt = (\n",
    "        \"現在開始使用我提供的情境來回答，只能使用繁體中文，不要有簡體中文字。如果你不確定答案，就說不知道。情境如下：\"\n",
    "        + \"\\n\".join(paragraphs[vector[0]] for vector in similar_vectors)\n",
    "    )\n",
    "\n",
    "    response = client.chat(\n",
    "        model=\"gemma2:27b\",\n",
    "        messages= [{'role': 'system', 'content': system_prompt},\n",
    "                   {'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "\n",
    "    print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
